---
tags:
  - 数据分析
  - NumPy
  - Pandas
author: WangXinYi
img: /images/homePage/数据分析.webp
top: false
summary: 陈旸-数据分析实战45讲
categories:
  - 数据分析
  - NumPy
  - Pandas
typora-root-url: ..
date: 2022-10-24 12:06:57
---

# 数据分析前言

## 白话数据概念

### 一、商业智能、数据仓库、数据挖掘

比如你认识了两个漂亮的女孩。

**商业智能**会告诉你要追哪个？成功概率有多大？

**数据仓库**会说，我这里存储了这两个女孩的相关信息，你要吗？

其中每个女孩的数据都有单独的文件夹，里面有她们各自的姓名、生日、喜好和联系方式等，这些具体的信息就是**数据元**，加起来叫作**元数据**。

**数据挖掘**会帮助你确定追哪个女孩，并且整理好数据仓库，这里就可以使用到各种算法，帮你做决策了。

### 二、数据挖掘中的类型

你可能会用到**分类算法**。御姐、萝莉、女王，她到底属于哪个分类？

如果认识的女孩太多了，多到你已经数不过来了，比如说5万人！你就可以使用**聚类算法**了，它帮你把这些女孩分成多个群组，比如5个组。然后再对每个群组的特性进行了解，进行决策。这样就把5万人的决策，转化成了5个组的决策。成功实现降维，大大提升了效率。

如果你想知道这个女孩的闺蜜是谁，那么**关联分析算法**可以告诉你。

### 三、数据预处理中的术语

如果你的数据来源比较多，比如有很多朋友给你介绍女朋友，很多人都推荐了同一个，你就需要去重，这叫**数据清洗**；

为了方便记忆，你把不同朋友推荐的同一个女孩信息在数据库中合成一个，这叫**数据集成**；

有些数据渠道统计的体重的单位是公斤，有些是斤，你就需要将它们转换成同一个单位，这叫**数据变换。**

最后你可以进行**数据可视化**了，它会直观地把你想要的结果呈现出来。

### 四、数据处理中的流程

#### 1、数据采集  

​	数据源 ：开源数据源、爬虫抓取、日志采集、传感器

​	工具使用：火车采集器、八爪鱼、搜集客

+ [火车采集器 	](http://www.locoy.com/)不仅可以做抓取工具，也可以做数据清洗、数据分析、数据挖掘和可视化等工作。数据源适用于绝大部分的网页，网页中能看到的内容都可以通过采集规则进行抓取。

+ [八爪鱼](http://www.bazhuayu.com/)    是知名的采集工具，它有两个版本，一个就是免费的采集模板，还有一个就是云采集（付费）。

  免费的采集模板实际上就是内容采集规则，包括了电商类、生活服务类、社交媒体类和论坛类的网站都可以采集，用起来非常方便。

  云采集就是当你配置好采集任务，就可以交给八爪鱼的云端进行采集。八爪鱼一共有5000台服务器，通过云端多节点并发采集，采集速度远远超过本地采集。此外还可以自动切换多个 IP，避免IP被封，影响采集。做过工程项目的同学应该能体会到，云采集这个功能太方便了，很多时候自动切换IP以及云采集才是自动化采集的关键。

+ [集搜客](http://www.gooseeker.com/)   这个工具的特点是完全可视化操作，无需编程。整个采集过程也是所见即所得，抓取结果信息、错误信息等都反应在软件中。相比于八爪鱼来说，集搜客没有流程的概念，用户只需要关注抓取什么数据，而流程细节完全交给集搜客来处理。但是集搜客的缺点是没有云采集功能，所有爬虫都是在用户自己电脑上跑的。

#### 2、数据挖掘 

​	找到其中的规律，来指导公司业务，数据挖掘的核心是挖掘数据的商业价值

+ 商业理解：数据挖掘不是我们的目的，我们的目的是更好地帮助业务，所以首先要从商业的角度理解项目需求，在这个基础上，再对数据挖掘的目标进行定义。 
+ 数据理解：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证等，这有助于你对收集的数据有个初步的认知。 
+ 数据准备：开始收集数据，并对数据进行清洗、数据集成等操作，完成数据挖掘前的准备工作。 
+ 模型建立：选择和应用各种数据挖掘模型，并进行优化，以便得到更好的分类结果。 
+ 模型评估：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现了预定的 商业目标。 
+ 上线发布：模型的作用是从数据中找到金矿，也就是我们所说的“知识”，获得的知识需要转化成用户可以使用的方式，数据挖掘结果如果是日常运营的一部分，那么后续的监控和维护就会变得重要。

#### 3、数据可视化

第一种：使用 Python，在 Python 对数据进行清洗、挖掘的过程中，我们可以使用 Matplotlib、Seaborn 等第三方库进行呈现。 

第二种：直接使用第三方工具。如果你已经生成了 csv 格式文件，可以采用微图、DataV、Data GIF Maker 等第三方工具。

## NumPy

​		标准的 Python 中，用列表list 保存数组的数值。由于列表中的元素可以是任意 的对象，所以列表中 list 保存的是对象的指针。虽然在 Python 编程中隐去了指针的概 念，但是数组有指针，Python 的列表 list 其实就是数组。这样如果我要保存一个简单的数组 [0,1,2]，就需要有 3 个指针和 3 个整数的对象，这样对于 Python 来说是非常不经济的，浪费了内存和计算时间。

​	为什么使用 NumPy 让你的 Python 科学计算更高效

1. 列表 list 的元素 在系统内存中是分散存储的，而 NumPy 数组存储在一个均匀连续的内存块中。这样数组计算遍历所有的元素，不像列表 list 还需要对内存地址进行查找，从而节省了计算资源。
2. 在内存访问模式中，缓存会直接把字节块从 RAM 加载到 CPU 寄存器中。因为数据连续的存储在内存中，NumPy 直接利用现代 CPU 的矢量化指令计算，加载寄存器中的多个连续浮点数。
3. 另外 NumPy 中的矩阵计算可以采用多线程的方式，充分利用多核 CPU 计算资源，大大提升了计算效率

在 NumPy 里有两个重要的对象： ndarray解决了多维数组问题，而 ufunc则是解决对数组进行处理的函数。

### 一、Ndarray

​	Ndarray全称为N-dimensional array object, 实际上是多维数组的含义。在 NumPy 数组中，维数称为秩（rank），一维数组 的秩为 1，二维数组的秩为 2，以此类推。在 NumPy 中，每一个线性的数组称为一个轴 （axes），其实秩就是描述轴的数量。

​	可以通过函数 shape 属性获得数组的大小，通过 dtype 获得元素的属性，

#### 1、Shape&dtype

```python
import numpy as np
a = np.array([1, 2, 3])
b = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
b[1,1]=10
print(a.shape)
print(b.shape)
print(a.dtype)
print(b)
```

运行结果：

```txt
(3,)
(3, 3)
int32
[[ 1  2  3]
 [ 4 10  6]
 [ 7  8  9]]
```

#### 2、结构数组

​	如果你想统计一个班级里面学生的姓名、年龄，以及语文、英语、数学成绩该怎么办？当然你可以用数组的下标来代表不同的字段，比如下标为 0 是姓名、下标为1是年龄等，但是这样不显性，此时便可以使用Numpy中的结构数组。

```python
persontype = np.dtype({'names':['name', 'age', 'chinese', 'math', 'english'],
			 'formats':['S32','i', 'i', 'i', 'f']})
peoples = np.array([("ZhangFei",32,75,100, 90),("GuanYu",24,85,96,88.5),
          ("ZhaoYun",28,85,92,96.5),("HuangZhong",29,65,85,100)],dtype=persontype)
ages = peoples[:]['age']
print(ages)
print(np.mean(ages))
```

运行结果：

```
[32 24 28 29]
28.25
```

### 二、Ufunc

#### 1、加、减、乘、除、指数、取余

```python
#arange 通过指定初始值、终值、步长来创建等差数列的一维数组，默认不包括终值
x1 = np.arange(1,11,2)
#linspace 通过指定初始值、终值、元素个数来创建等差数列的一维数组，默认是包括终值的
x2 = np.linspace(1,9,5)
print(x1)
print(x2)
print(np.add(x1, x2)) 		#加
print(np.subtract(x1, x2)) 	#减
print(np.multiply(x1, x2))  #乘
print(np.divide(x1, x2)) 	#除
print(np.power(x1, x2)) 	#指数
print(np.mod(x1, x2)) 		#取余
```

运行结果：

```
[1 3 5 7 9]
[1. 3. 5. 7. 9.]
[ 2.  6. 10. 14. 18.]
[0. 0. 0. 0. 0.]
[ 1.  9. 25. 49. 81.]
[1. 1. 1. 1. 1.]
[1.00000000e+00 2.70000000e+01 3.12500000e+03 8.23543000e+05 3.87420489e+08]
[0. 0. 0. 0. 0.]
```

#### 2、最大值、最小值

amin() 用于计算数组中的元素沿指定轴的最小值, amax()同理。

amin(a,0) 是延着 axis=0 轴的最小值，axis=0 轴是把元素看成了 [1,4,7], [2,5,8], [3,6,9] 三个元素。

amin(a,1) 是延着 axis=1 轴的最小值，axis=1 轴是把元素看成了 [1,2,3], [4,5,6], [7,8,9] 三个元素。

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print(np.amin(a))
print(np.amin(a,0))
print(np.amin(a,1)) 
```

运行结果：

```
1
[1 2 3]
[1 4 7]
```

#### 3、最大值与最小值之差

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print(np.ptp(a))
print(np.ptp(a,0))
print(np.ptp(a,1))
```

运行结果：

```
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print(np.ptp(a))
print(np.ptp(a,0))
print(np.ptp(a,1))
```

#### 4、统计数组的百分位数&中位数

percentile() 代表着第 p 个百分位数，这里 p 的取值范围是 0-100，如果 p=0， 那么就是求最小值，如果 p=50 就是求平均值，如果 p=100 就是求最大值。同样你也可 以求得在 axis=0 和 axis=1 两个轴上的 p% 的百分位数。

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print(np.percentile(a, 50))
print(np.percentile(a, 50, axis=0))
print(np.percentile(a, 50, axis=1))
# 求中位数
print(np.median(a))
print(np.median(a, axis=0))
print(np.median(a, axis=1))
```

运行结果：

```
5.0
[4. 5. 6.]
[2. 5. 8.]
# 求中位数
5.0
[4. 5. 6.]
[2. 5. 8.]
```

#### 5、统计数组中的加权平均值

average() 函数可以求加权平均，加权平均的意思就是每个元素可以设置个权重，默认情况下每个元素的权重是相同的，所以 np.average(a)=(1+2+3+4)/4=2.5，你也可以指定 权重数组 wts=[1,2,3,4]，这样加权平均 np.average(a,weights=wts)= (1*1+2*2+3*3+4*4)/(1+2+3+4)=3.0。

```python
a = np.array([1,2,3,4])
wts = np.array([1,2,3,4])
print(np.average(a))
print(np.average(a,weights=wts))
```

运行结果：

```
2.5
3.0
```

#### 6、统计数组中的标准差、方差

方差的计算是指每个数值与平均值之差的平方求和的平均值，即 mean((x - x.mean())** 2)。标准差是方差的算术平方根。

```python
a = np.array([1,2,3,4])
print(np.std(a)) #标准差
print(np.var(a)) #方差
```

运行结果：

```
1.118033988749895
1.25
```

#### 7、排序

sort(a, axis=-1, kind=‘quicksort’, order=None)；

+ kind 字段：默认情况下使 用的是快速排序，可以指定 quicksort、mergesort、heapsort 分别表示快速排序、合并排序、堆排序。

+ axis 字段：默认是 -1，即沿着数组的最后一个轴进行排序， 也可以取不同的 axis 轴，或者 axis=None 代表采用扁平化的方式作为一个向量进行排序。
+ order 字段：对于结构化的数组可以指定按照某个字段进行排序。

```python
a = np.array([[4,3,2],[2,4,1]])
print(np.sort(a))
print(np.sort(a, axis=None))
print(np.sort(a, axis=0))
print(np.sort(a, axis=1))
```

运行结果：

```
[[2 3 4][1 2 4]]
[1 2 2 3 4 4]
[[2 3 1][4 4 2]]
[[2 3 4][1 2 4]]
```

## Pandas

在数据分析工作中，Pandas 的使用频率是很高的，一方面是因为 Pandas 提供的基础数据结构DataFrame 与 json 的契合度很高，转换起来就很方便。另一方面，如果我们数据清理工作不是很复杂的话，通常用几句 Pandas 代码就可以对数据进行规整。

Series 和 DataFrame 这两个核心数据结构，他们分别代表着一维的序列和二维的表结构。基于这两种数据结构，Pandas 可以对数据进行导入、清洗、处理、 统计和输出。

### 一、Series

Series 是个定长的字典序列。说是定长是因为在存储的时候，相当于两个 ndarray，这也是和字典结构最大的不同。因为在字典的结构里，元素的个数是不固定的。 

Series有两个基本属性：index 和 values。在 Series 结构中，index 默认是 0,1,2,……递增的整数序列，当然我们也可以自己来指定索引，比如 index=[‘a’, ‘b’, ‘c’, ‘d’]。

```python
import pandas as pd
from pandas import Series, DataFrame
x1 = Series([1,2,3,4])
x2 = Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd'])
print(x1)
print(x2)
```

运行结果：

```
0    1
1    2
2    3
3    4
dtype: int64
a    1
b    2
c    3
d    4
dtype: int64
```

我们也可以采用字典的方式来创建 Series

```python
d = {'a':1, 'b':2, 'c':3, 'd':4}
x3 = Series(d)
print(x3)
```

运行结果：

```
a    1
b    2
c    3
d    4
dtype: int64
```

### 二、DataFrame

**DataFrame 类型数据结构类似数据库表。** 

它包括了行索引和列索引，我们可以将 DataFrame 看成是由相同索引的 Series 组成的字 典类型。

一般会用 df, df1, df2 这些作为 DataFrame 数据类型的变量名，以例子中的 df2 为例，列索引是 [‘English’, ‘Math’, ‘Chinese’]，行索引是 [‘ZhangFei’, ‘GuanYu’, ‘ZhaoYun’, ‘HuangZhong’, ‘DianWei’]。

```python
data = {'Chinese': [66, 95, 93, 90,80],'English': [65, 85, 92, 88, 90],'Math': [30, 98, 96, 77, 90]}
df1= pd.DataFrame(data)
df2 = pd.DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei'], columns=['English', 'Math', 'Chinese'])
print(df1)
print(df2)
```

运行结果：

```
 Chinese  English  Math
0       66       65    30
1       95       85    98
2       93       92    96
3       90       88    77
4       80       90    90
            English  Math  Chinese
ZhangFei         65    30       66
GuanYu           85    98       95
ZhaoYun          92    96       93
HuangZhong       88    77       90
DianWei          90    90       80
```

### 三、用DataFranme进行数据处理

#### 1、数据导入和输出

需要先下载依赖

```
pip install	xlrd
pip install	openpyxl
```

将data.xlsx放在Jupyter目录下,默认为 C:\Users\\${用户名}

```python
score = DataFrame(pd.read_excel('data.xlsx'))
score.to_excel('data1.xlsx')
print(score)
```

#### 2、数据清洗

以上文df2为例

**删除 DataFrame 中的不必要的列或行**

```python
df2 = df2.drop(columns=['Chinese'])
df2 = df2.drop(index=['ZhangFei'])
```

**重命名列名 columns，让列表名更容易识别**

```python
df2.rename(columns={'Chinese': 'YuWen', 'English': 'Yingyu'}, inplace = True)
```

**去除重复的值**

不是字段名相同就去除，而是这个字段里面的数组一样才去除

```python
df = df2.drop_duplicates()#去除重复行
```

**更改数据格式**

把 Chinese 字段的值改成 str 类型，或者 int64 可以这么写

```python
df2['Chinese'].astype('str')
df2['Chinese'].astype(np.int64)
```

**数据间的空格**

有时候我们先把格式转成了 str 类型，是为了方便对数据进行操作，这时想要删除数据间的空格或字符，我们就可以使用 strip 函数

```python
# 删除左右两边空格
df2['Chinese']=df2['Chinese'].map(str.strip)
# 删除左边空格
df2['Chinese']=df2['Chinese'].map(str.lstrip)
# 删除右边空格
df2['Chinese']=df2['Chinese'].map(str.rstrip)
# 删除其中的$符号
df2['Chinese']=df2['Chinese'].str.strip('$')
```

**大小写转换**

```python
#全部大写
df2.columns = df2.columns.str.upper()
#全部小写
df2.columns = df2.columns.str.lower()
#首字母大写
df2.columns = df2.columns.str.title()
```

**查找空值**

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221025092409470.png" alt="表结构" style="zoom:50%;box-shadow:rgba(0,0,0,0) 0 0px 0px 0px;" />

针对数据表df使用df.isnull()

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221025092428614.png"  style="zoom:50%;" />



针对某列使用df.isnull().any()

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221025092518068.png"  style="zoom:67%;" />

**使用 apply 函数对数据进行清洗**

​	apply 函数是 Pandas 中自由度非常高的函数，使用频率也非常高。

​	我们也可以定义个函数，在 apply中进行使用。比如定义 double_df 函数是将原来的数值 *2 进行返回。然后对 df1 中的“语文”列的数值进行 *2 处理，可以写成：

```python
def double_df(x):
	return 2*x
df1[u'语文'] = df1[u'语文'].apply(double_df)
```

#### 3、数据统计

**统计函数**

​	describe() 函数最简便。它是个统计大礼包，可以快速让我们对数据有个全面的了解。

```python
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
print(df1.describe())
```

运行结果：

```
          data1
count  5.000000
mean   2.000000
std    1.581139
min    0.000000
25%    1.000000
50%    2.000000
75%    3.000000
max    4.000000
```

**数据表合并**

​	基于指定列进行连接

```python
df3 = pd.merge(df1, df2, on='name')
```

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221025093942797.png" alt="指定字段连接"  style="zoom: 50%;" />	

​	inner 内连接

```python
df3 = pd.merge(df1, df2, how='inner')
```

​	<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221025094218284.png" alt="两表的交集" style="zoom:50%;" />

left 左连接

```python
df3 = pd.merge(df1, df2, how='left')
```

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221025094330227.png" alt="左表为主,右表补充" style="zoom:50%;" />	

right 右连接

```python
df3 = pd.merge(df1, df2, how='right')
```

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221025094517630.png" alt="右表为主,左表补充" style="zoom:50%;" />	

outer 外连接

```python
df3 = pd.merge(df1, df2, how='outer')
```

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221025094558773.png" alt="两表的并集" style="zoom:50%;" />

**用 SQL 方式打开 Pandas**

pandasql 中的主要函数是 sqldf，它接收两个参数：一个 SQL 查询语句，还有一组环境 变量 globals() 或 locals()。这样我们就可以在 Python 里，直接用 SQL 语句中对 DataFrame 进行操作

```python
import pandas as pd
from pandas import DataFrame
from pandasql import sqldf, load_meat, load_births
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
pysqldf = lambda sql: sqldf(sql, globals())
sql = "select * from df1 where name ='ZhangFei'"
print pysqldf(sql)
```

 lambda函数是用来定义一个匿名函数的，具体的使用形式为：

```
lambda argument_list: expression
```

这里 argument_list 是参数列表，expression 是关于参数的表达式，会根据 expression 表达式计算结果进行输出返回。 在上面的代码中，我们定义了：

```python
pysqldf = lambda sql: sqldf(sql, globals())
```

在这个例子里，输入的参数是 sql，返回的结果是 sqldf 对 sql 的运行结果，当然 sqldf 中也输入了 globals 全局参数，因为在 sql 中有对全局参数 df1 的使用。

## Pytorch

### 一、Pytorch 和 Numpy转换

```python
import torch
import numpy as np
np_data = np.arange(6).reshape((2,3))
torch_data = torch.from_numpy(np_data)
tensor2array = torch_data.numpy()
print(
	'\nnumpy',np_data
	'\ntorch',torch_data,
)
```

### 二、Variable

### 1、torch.FloatTensor()函数

类型转换, 将list ,numpy转化为tensor。 以list -> tensor为例：

```python
print(torch.FloatTensor([1,2]))
# 输出: tensor([1., 2.])
```

根据torch.Size()创建一个空tensor

```python
a = torch.tensor([[1, 2], [3, 4]])
print(torch.FloatTensor(a.size()))
```

输出如下

```python
tensor([[ 9.4636e+33,  4.5559e-41],
        [-3.9725e-26,  3.0631e-41]])
```

### 2、常见的激励函数

```
import torch
//Relu

//
```

### 3、基本的网络结构

```python
# 方法 1 
class Net(torch.nn.Module):
	def _init_(self, ...):
		super(Net, self)._init_();
		...
	def forward(self, x):
		...
# 方法 2
	net2 = torch.nn.Sequential(
    	torch.nn.Linear(2,10),
        torch.nn.ReLU(),
        torch.nn.Linear(10,2),
    )
    
# CNN
class CNN(torch.nn.Module):
    def __init__(self):
        super(CNN,self).__init__()
        self.conv1 = nn.Sequential(
        	nn.Con2d(
            	in_channels = 1,
                out_channels = 16,
                kernel.size = 5,
                stride = 1,
                padding = 2,
            ),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
        )
        self.conv2 = nn.Sequential(
        	nn.Conv2d(16,32,5,1,2),
            ...
        )
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(X) # (batch, 32, 7, 7)
        x = x.view(x.size(0), -1) # (batch, 32 * 7 * 7)
        output = self.out(x)
        return ouput
```

### 4、优化器

```python
#lr：学习效率，一般都小于1
optimizer = torch.optim.SGD(net.parameters(), lr=0.5)
#回归计算误差MSELoss，分类计算误差 CrossEntropyLoss
loss_func = torch.nn.MSELoss();

#训练步数
for t int range(100):
    #预测结果
    prediction = net(x)
    #预测计算误差
    loss = loss_func(prediction, y)
    #优化器梯度清零、反向传播、把反向传播的参数施加到网络里
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

###  5、保存提取神经网络

```python
#保留
def save:
    #保留整个网络
    torch.save(net1, 'net.pkl')
    #保留参数
    torch.save(net1.state_dict(), 'net_params.pkl')
#提取整个网络
def restore_net():
	net2 = torch.load('net.pkl')
#提取参数
def restore_params():
	net3 = torch.nn.Sequential(
    	torch.nn.Linear(2,10),
        torch.nn.ReLU(),
        torch.nn.Linear(10,2),
    )
    net3.load_state_dict(torch.lod('net_params.pkl'))
```

### 6、批处理

```python
torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)
loader = Data.DataLoader(
	dataset = torch_dataset, # 数据集
	batch_size = BATCH_SIZE, # 批处理大小
	shuffle = Ture,	 # 打乱
	num_workers = 2, # 读的进程数
)

for epoch in range(3):
    for step,(batch_x, batch_y) in enumerate(loader):
        # training...
        print('Epoch:',epoch,'|Step:',step,
              '|batch x:',batch_x.numpy(),'|batch y:',batch_y.numpy())
```

### 7、Dropout

```python
net_dropped = torch.nn.Sequential(
    torch.nn.Linear(2,10),
    torch.nn.Dropout(0.5)
    torch.nn.ReLU(),
    torch.nn.Linear(10,2),
)
```



# 数据预处理与算法

## 用户画像建模

### 一、设计唯一标识

​	**用户唯一标识是整个用户画像的核心**。我们以一个App为例，它把“从用户开始使用APP到下单到售后整个所有的用户行为”进行串联，这样就可以更好地去跟踪和分析一个用户的特征。设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID等。

### 二、用户标签化

+ 用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性。
+ 消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。
+ 行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用App的习惯。
+ 内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等。

### 三、赋能业务价值

+ 获客：如何进行拉新，通过更精准的营销获取客户。
+ 粘客：个性化推荐，搜索排序，场景运营等。
+ 留客：流失率预测，分析关键节点降低流失率。

### 四、以美团为例

#### 1、以美团为例设计唯一标识

+ 手机号是唯一标识

#### 2、以美团为例用户标签化

+ 用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。
+ 消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。
+ 行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。
+ 内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。

#### 3、以美团为例赋能业务价值

+ 在获客上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。
+ 在粘客上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。
+ 留客上，预测用户是否可能会从平台上流失。在营销领域，关于用户留存有一个观点——如果将顾客流失率降低5%，公司利润将提升25%~85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。

## 数据预处理

### 一、数据清洗

我将数据清洗规则总结为以下4个关键点，统一起来叫“**完全合一**”。

1. **完**整性：单条数据是否存在空值，统计的字段是否完善。
2. **全**面性：观察某一列的全部数值，比如在Excel表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。
3. **合**法性：数据的类型、内容、大小的合法性。比如数据中存在非ASCII字符，性别存在了未知，年龄超过了150岁等。
4. 唯**一**性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。

按照以上的原则，我们对以下图片进行数据清洗

​	这是一家服装店统计的会员数据。最上面的一行是列坐标，最左侧一列是行坐标。

​	列坐标中，第0列代表的是序号，第1列代表的会员的姓名，第2列代表年龄，第3列代表体重，第4-6列代表男性会员的三围尺寸，第7-9列代表女性会员的三围尺寸。

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221028195420586.png" alt="需要数据清洗" style="zoom:80%;" />

下面，我们就依照“完全合一”的准则，使用Pandas来进行清洗。

#### 1、完整性

**问题1：缺失值**

在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：

+ 删除：删除数据缺失的记录；
+ 均值：使用当前列的均值；
+ 高频：使用当前列出现频率最高的数据。

比如我们想对df[‘Age’]中缺失的数值用平均年龄进行填充，可以这样写：

```python
df['Age'].fillna(df['Age'].mean(), inplace=True)
```

如果我们用最高频的数据进行填充，可以先通过value_counts获取Age字段最高频次age_maxf，然后再对Age字段中缺失的数据用age_maxf进行填充：

```python
age_maxf = train_features['Age'].value_counts().index[0] train_features['Age'].fillna(age_maxf, inplace=True)
```

**问题2：空行**

我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv() 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna() 进行处理，删除空行。

```python
df.dropna(how='all',inplace=True) # 删除全空的行
```

#### 2、全面性

**问题：列数据的单位不统一**

观察weight列的数值，我们能发现weight 列的单位不统一。有的单位是千克（kgs），有的单位是磅（lbs）。

这里我使用千克作为统一的度量单位，将磅（lbs）转化为千克（kgs）：

```python
# 获取 weight 数据列中单位为 lbs 的数据
rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
print df[rows_with_lbs]
# 将 lbs转换为 kgs, 2.2lbs=1kgs
for i,lbs_row in df[rows_with_lbs].iterrows():
	# 截取从头开始到倒数第三个字符之前，即去掉lbs。
	weight = int(float(lbs_row['weight'][:-3])/2.2)
	df.at[i,'weight'] = '{}kgs'.format(weight) 
```

#### 3、合理性

**问题：非ASCII字符**

我们可以看到在数据集中 Fristname 和 Lastname （在唯一性中将名字拆分为包含了两个字段Firtname 和 Lastname）有一些非 ASCII 的字符。我们可以采用删除或者替换的方式来解决非ASCII问题，这里我们使用删除方法：

```python
df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True) # 删除非 ASCII 字符
df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
```

#### 4、唯一性

**问题1：一列有多个参数**

在数据中不难发现，姓名列（Name）包含了两个参数 Firtname 和 Lastname。为了达到数据整洁目的，我们将 Name 列拆分成 Firstname 和 Lastname两个字段。我们使用Python的split方法，str.split(expand=True)，将列表拆成新的列，再将原来的 Name 列删除。

```python
df[['first_name','last_name']] = df['name'].str.split(expand=True) # 切分名字，删除源数据列
df.drop('name', axis=1, inplace=True)
```

**问题2：重复数据**

我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop_duplicates() 来删除重复数据。

```python
df.drop_duplicates(['first_name','last_name'],inplace=True) # 删除重复数据行
```

最终效果图：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221028200303888.png" alt="数据清洗效果图" style="zoom: 50%;" />

### 二、数据集成

#### 1、数据集成的两种架构：ELT和ETL

ETL 的过程为提取(Extract)——转换(Transform)——加载(Load)，在数据源抽取后首先进行转换，然后将转换的结果写入目的地。

ELT 的过程则是提取(Extract)——加载(Load)——变换(Transform)，在抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如Spark来完成转换的步骤。

目前数据集成的主流架构是ETL，但未来使用ELT作为数据集成架构的将越来越多。这样做会带来多种好处：

+ ELT 和 ETL 相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量的方案搭建起一个数据集成平台。使用 ELT 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面ELT允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。
+ 在ELT架构中，数据变换这个过程根据后续使用的情况，需要在 SQL 中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程。

### 三、数据变换

举个例子，假设A考了80分，B也考了80分，但前者是百分制，后者500分是满分，如果我们把从这两个渠道收集上来的数据进行集成、挖掘，就算使用效率再高的算法，结果也不是正确的。因为这两个渠道的分数代表的含义完全不同。

所以说，有时候数据变换比算法选择更重要，数据错了，算法再正确也是错的。那么如何让不同渠道的数据统一到一个目标数据库里呢？这样就用到了数据变换。

数据变换是数据准备的重要环节，它**通过数据平滑、数据聚集、数据概化和规范化等方式**将数据转换成适用于数据挖掘的形式。

#### 1、常见的数据变换方法

+ **数据平滑**：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑；
+ **数据聚集**：对数据进行汇总，在SQL中有一些聚集函数可以供我们操作，比如Max()反馈某个字段的数值最大值，Sum()返回某个字段的数值总和；
+ **数据概化**：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。
+ **数据规范化**：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等；
+ **属性构造**：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。

在这些变换方法中，最简单易用的就是对数据进行规范化处理，下面介绍数据进行规范化处理的几种方法，并使用Python中的SciKit-Learn进行数据规范化。

#### 2、Min-max 规范化

Min-max标准化方法是将原始数据变换到[0,1]的空间中。用公式表示就是：新数值=（原数值-极小值）/（极大值-极小值）。

我们可以让原始数据投射到指定的空间[min, max]，在SciKit-Learn里有个函数MinMaxScaler是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到[min, max]中。默认情况下[min,max]是[0,1]，也就是把原始数据投放到[0,1]范围内。

我们来看下下面这个例子：

```python
# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化
min_max_scaler = preprocessing.MinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print minmax_x
```

运行结果：

```
[[0.         0.         0.66666667]
 [1.         1.         1.        ]
 [0.         1.         0.        ]]
```

#### 3、Z-Score 规范化

我们定义：新数值=（原数值-均值）/ 标准差。

Z-Score的优点是算法简单，不受数据量级影响，结果易于比较。不足在于，它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。在SciKit-Learn库中使用preprocessing.scale()函数，可以直接将给定数据进行Z-Score规范化。

```python
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行Z-Score规范化
scaled_x = preprocessing.scale(x)
print scaled_x
```

运行结果：

```
[[-0.70710678 -1.41421356  0.26726124]
 [ 1.41421356  0.70710678  1.06904497]
 [-0.70710678  0.70710678 -1.33630621]]
```

这个结果实际上就是将每行每列的值减去了平均值，再除以方差的结果。

我们看到Z-Score规范化将数据集进行了规范化，数值都符合均值为0，方差为1的正态分布。

#### 4、小数定标规范化

小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性A的取值中的最大绝对值。

举个例子，比如属性A的取值范围是-999到88，那么最大绝对值为999，小数点就会移动3位，即新数值=原数值/1000。那么A的取值范围就被规范化为-0.999到0.088。我们需要用NumPy库来计算小数点的位数。

这里我们看下运行代码：

```python
# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 标准差标准化
j = np.ceil(np.log10(np.max(abs(x))))
scaled_x = x/(10**j)
print scaled_x
```

运行结果：

```
[[ 0.  -0.3  0.1]
 [ 0.3  0.1  0.2]
 [ 0.   0.1 -0.1]]
```

 

另外的专题

python爬虫

python可视化

通过决策树来进行泰坦尼克乘客生存预测



## 算法四大分类 

### 一、分类算法 

就是通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类，分类需要知道事先的定义，常见分类算法：C4.5、CART、朴素贝叶斯、SVM、KNN、AdaBoost

### 二、聚类算法 

聚类就是将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做数据划分，往往聚类不知道事先的定义，常见聚类算法：K-Means、EM

### 三、关联分析 

通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险，比如在A出现10次同时， B出现8次，常见关联分析算法：Apriori

### 四、连接分析  

就是发现数据中的关联规则，它被广泛应用在购物篮分析，或事务数据分析中，比如通过邮件来连接，常见连接分析算法：PageRank

## 算法十大模型

### 一、决策树

总结：决策树的原理就是找到**纯度最高的点作为根结点**，并继续通过**信息增益**或**基尼系数**判断剩下结点那个纯度最高，继续选择根节点。直到递归终止到分到某个类时，目标属性全是一个值或者某个值的比例达到给定的阈值

讲的很好的相关视频：[【数据挖掘】决策树零基础入门教程，手把手教你学决策树！_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1T7411b7DG/?spm_id_from=333.337.search-card.all.click&vd_source=178669e0797f36fb015c5fc3cb9c7a4f)

#### 1、纯度、信息熵

​	决策树的在决策过程中有三个重要的问题：

+ 将哪个属性作为根节点？
+ 选择哪些属性作为后继节点？
+ 什么时候停止并得到目标值？

在这里我们先介绍两个指标：**纯度**和**信息熵**。

先来说一下纯度，你可以把决策树的构造过程理解成为寻找纯净划分的过程。

数学上，我们可以用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。我在这里举个例子，假设有3个集合：

+ 集合1：6次都去打篮球；
+ 集合2：4次去打篮球，2次不去打篮球；
+ 集合3：3次去打篮球，3次不去打篮球。

按照纯度指标来说，集合1>集合2>集合3。因为集合1的分歧最小，集合3的分歧最大。

然后我们再来介绍信息熵（entropy）的概念，它表示了信息的不确定度。

在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221029102814947.png" alt="信息熵的计算" style="zoom:50%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;"/>

p(i|t)代表了节点t为分类i的概率，其中log2为取以2为底的对数。它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。**信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。**

在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3算法）、信息增益率（C4.5算法）以及基尼指数（Cart算法）。

+ 首先ID3算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。
+ C4.5在ID3的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于C4.5需要对数据集进行多次扫描，算法效率相对较低。
+ CART只支持二叉树，CART分类树是基于基尼系数做判断，CART回归树是基于偏差做判断。

决策有关的计算视频：[【决策树算法计算】ID3算法 C4.5算法 Cart算法 相关计算 _哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Cq4y1S7k1/?spm_id_from=333.788.recommend_more_video.-1&vd_source=178669e0797f36fb015c5fc3cb9c7a4f)

#### 2、ID3算法

**总结：使用信息增益最大的作为根节点**

ID3算法计算的是**信息增益**，信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221029102517289.png" alt="信息增益的计算" style="zoom: 50%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

#### 3、C4.5算法

**总结：使用信息增益率最大的作为根节点** 

在ID3算法上进行改进的C4.5算法，那么C4.5都在哪些方面改进了ID3呢？

+ **采用信息增益率**

因为ID3在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5采用信息增益率的方式来选择属性。信息增益率=信息增益/属性熵，具体的计算公式这里省略。当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于C4.5来说，属性熵也会变大，所以整体的信息增益率并不大。

+ **采用悲观剪枝**

ID3构造决策树的时候，容易产生过拟合的情况。在C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。

+ **离散化处理连续属性**

C4.5可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，**C4.5选择具有最高信息增益的划分所对应的阈值**。

+ **处理缺失值**

针对数据集不完整的情况，C4.5也可以进行处理。

#### 4、Cart算法

**总结：CART分类树是基尼系数最小的作为根节点，CART回归树是基于偏差做判断。**

CART算法，英文全称叫做Classification And Regression Tree，中文叫做分类回归树。ID3和C4.5算法可以生成二叉树或多叉树，而CART只支持二叉树。同时CART决策树比较特殊，既可以作分类树，又可以作回归树。**在工具使用上，我们可以使用sklearn中的DecisionTreeClassifier创建CART分类树，通过DecisionTreeRegressor创建CART回归树。**

+ CART分类二叉树

分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别。作为分类树，CART采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；假设t为节点，那么该节点的GINI系数的计算公式为：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221029130731142.png" alt="基尼系数的计算" style="zoom: 33%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

这里p(C<SUB>k</SUB>|t)表示节点t属于类别C<SUB>k</SUB>的概率，节点t的基尼系数为1减去各类别C<SUB>k</SUB>概率平方和。

下面，我们来用CART分类树，给sklearn中自带的iris数据集构造一棵分类决策树。

+ 首先train_test_split可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。
+ 使用clf = DecisionTreeClassifier(criterion=‘gini’)初始化一棵CART分类树。这样你就可以对CART分类树进行训练。
+ 使用clf.fit(train_features, train_labels)函数，将训练集的特征值和分类标识作为参数进行拟合，得到CART分类树。
+ 使用clf.predict(test_features)函数进行预测，传入测试集的特征值，可以得到测试结果test_predict。
+ 最后使用accuracy_score(test_labels, test_predict)函数，传入测试集的预测结果与实际的结果作为参数，得到准确率score。
+ 我们能看到sklearn帮我们做了CART分类树的使用封装，使用起来还是很方便的。

基于iris数据集，构造CART分类树的代码如下：

```python
# encoding=utf-8
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
# 准备数据集
iris=load_iris()
# 获取特征集和分类标识
features = iris.data
labels = iris.target
# 随机抽取33%的数据作为测试集，其余为训练集
train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)
# 创建CART分类树
clf = DecisionTreeClassifier(criterion='gini')
# 拟合构造CART分类树
clf = clf.fit(train_features, train_labels)
# 用CART分类树做预测
test_predict = clf.predict(test_features)
# 预测结果与测试集结果作比对
score = accuracy_score(test_labels, test_predict)
print("CART分类树准确率 %.4lf" % score)
```

运行结果：

```
CART分类树准确率 0.9600
```

+ CART回归树二叉树

​     回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。CART可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设x为样本的个体，均值为u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。

其中差值的绝对值为样本值减去样本均值的绝对值：

<img src="https://static001.geekbang.org/resource/image/6f/97/6f9677a70b1edff85e9e467f3e52bd97.png" alt="最小绝对偏差（LAD）" style="zoom:33%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

方差为每个样本值减去样本均值的平方和除以样本个数：

<img src="https://static001.geekbang.org/resource/image/04/c1/045fd5afb7b53f17a8accd6f337f63c1.png" alt="最小二乘偏差（LSD）" style="zoom: 67%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。

这里我们使用到sklearn自带的波士顿房价数据集，该数据集给出了影响房价的一些指标，比如犯罪率，房产税等，最后给出了房价。根据这些指标，我们使用CART回归树对波士顿房价进行预测，

+ 首先加载了波士顿房价数据集，得到特征集和房价。然后通过train_test_split帮助我们把数据集抽取一部分作为测试集，其余作为训练集。
+ 使用dtr=DecisionTreeRegressor()初始化一棵CART回归树。
+ 使用dtr.fit(train_features, train_price)函数，将训练集的特征值和结果作为参数进行拟合，得到CART回归树。
+ 使用dtr.predict(test_features)函数进行预测，传入测试集的特征值，可以得到预测结果predict_price。
+ 最后我们可以求得这棵回归树的二乘偏差均值，以及绝对值偏差均值。
+ 我们能看到CART回归树的使用和分类树类似，只是最后求得的预测值是个连续值。

代码如下：

```python
# encoding=utf-8
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error
from sklearn.tree import DecisionTreeRegressor
# 准备数据集
boston=load_boston()
# 探索数据
print(boston.feature_names)
# 获取特征集和房价
features = boston.data
prices = boston.target
# 随机抽取33%的数据作为测试集，其余为训练集
train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)
# 创建CART回归树
dtr=DecisionTreeRegressor()
# 拟合构造CART回归树
dtr.fit(train_features, train_price)
# 预测测试集中的房价
predict_price = dtr.predict(test_features)
# 测试集的结果评价
print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))
print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) 
```

运行结果（每次运行结果可能会有不同）：

```
['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO' 'B' 'LSTAT']
回归树二乘偏差均值: 23.80784431137724
回归树绝对值偏差均值: 3.040119760479042
```

5、决策树实战

​	通过决策树来进行泰坦尼克乘客生存预测

### 二、朴素贝叶斯

​	对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为 这个未知物体属于哪个分类。

**先验概率**：通过经验来判断事情发生的概率

**后验概率**：后验概率就是发生结果之后，推测原因的概率。

**条件概率**：事件A 在另外一个事件B已经发生条件下的发生概率，表示为P(A|B)，读作“在B 发生的条件下A 发生的概率”。

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221029170040406.png" alt="贝叶斯公式" style="zoom:50%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

由此，我们可以得出通用的贝叶斯公式：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221029170146364.png" alt="通用贝叶斯公式" style="zoom: 50%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

#### 1、朴素贝叶斯

讲完贝叶斯原理之后，我们再来看下朴素贝叶斯。

**它是一种简单但极为强大的预测建模算法**。之所以称为朴素贝叶斯，是因为它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。朴素贝叶斯模型由两种类型的概率组成：

1. 每个类别的概率P(C<SUB>j</SUB>)；
2. 每个属性的条件概率P(A<SUB>i</SUB>|C<SUB>j</SUB>)。

#### 2、贝叶斯原理、贝叶斯分类和朴素贝叶斯的区别

贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。

朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。

#### 3、Sklearn机器学习包中的朴素贝叶斯

sklearn机器学习包给我们提供了3个朴素贝叶斯分类算法，分别是

+ **高斯朴素贝叶斯**（GaussianNB）：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。
+ **多项式朴素贝叶斯**（MultinomialNB）：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的TF-IDF值等。
+ **伯努利朴素贝叶斯**（BernoulliNB）：特征变量是布尔变量，符合0/1分布，在文档分类中特征是单词是否出现。

#### 4、TF-IDF（补充 ）

+ **词频TF**计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221029172135853.png" alt="词频TF" style="zoom:50%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

+ **逆向文档频率IDF**，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF越大就代表该单词的区分度越大。有些单词可能不会存在文档中，为了避免分母为0，统一给单词出现的文档数都加1。

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221029172151209.png" alt="逆向文档频率IDF" style="zoom:50%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

+ **所以TF-IDF实际上是词频TF和逆向文档频率IDF的乘积**。这样我们倾向于找到TF和IDF取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。

+ **如何求TF-IDF**，在sklearn中我们直接使用TfidfVectorizer类，它可以帮我们计算单词TF-IDF向量的值。在这个类中，取sklearn计算的对数log时，底数是e，不是10。

### 三、SVM

#### 1、SVM的原理

**SVM就是帮我们找到一个超平面**，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。

在这个过程中，**支持向量**就是离超平面最近的样本点，实际上如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实对分类都没有意义。

**对分类间隔的大小进行定义**，首先，我们定义某类样本集到超平面的距离是这个样本集合内的样本到超平面的最短距离。我们用d<SUB>i</SUB>代表点x<SUB>i</SUB>到超平面wx<SUB>i</SUB>+b=0的欧氏距离。因此我们要求d<SUB>i</SUB>的最小值，用它来代表这个样本到超平面的最短距离。d<SUB>i</SUB>可以用公式计算得出：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221029193733145.png" alt="分类间隔的大小" style="zoom: 30%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

我们的目标就是找出所有分类间隔中最大的那个值对应的超平面。在数学上，这是一个凸优化问题（凸优化就是关于求凸集中的凸函数最小化的问题，这里不具体展开），通过凸优化问题，最后可以求出最优的w和b，也就是我们想要找的最优超平面。

#### 2、核函数（相当于激活函数）

**核函数:它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分**。

所以在非线性SVM中，核函数的选择就是影响SVM最大的变量。最常用的核函数有线性核、多项式核、高斯核、拉普拉斯核、sigmoid核，或者是这些核函数的组合。这些函数的区别在于映射方式的不同。通过这些核函数，我们就可以把样本空间投射到新的高维空间中。

#### 3、硬间隔、软间隔和非线性SVM

**硬间隔**，假如数据是完全的线性可分的，那么学习到的模型可以称为硬间隔支持向量机。换个说法，硬间隔指的就是完全分类准确，不能存在分类错误的情况。

**软间隔**，就是允许一定量的样本分类错误，我们知道，实际工作中的数据没有那么“干净”，或多或少都会存在一些噪点，所以线性可分是个理想情况。这时，我们需要使用到软间隔SVM（近似线性可分）

当然软间隔和核函数的提出，都是为了方便我们对上面超平面公式中的w和b进行求解，从而得到最大分类间隔的超平面。

### 四、KNN

#### 1、KNN算法的定义

​	KNN 也叫 K 最近邻算法，就是每个样本都可以用它最接近的 K 个邻居来代表。如果一个样本，它的 K 个最接近的邻居都属于分类 A，那么这个样本也属于分类 A。原理如下：

1. 计算待分类物体与其他物体之间的距离；
2. 统计距离最近的K个邻居；
3. 对于K个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。

#### 2、KNN算法中K值的选择

你能看出整个KNN的分类过程，K值的选择还是很重要的。

+ 如果 K 值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样KNN分类就会产生过拟合。

+ 如果K值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。


所以K值应该是个实践出来的结果，并不是我们事先而定的。在工程上，我们一般采用交叉验证的方式选取 K 值。

交叉验证的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性。所以在KNN算法中，我们一般会把K值选取在较小的范围内，同时在验证集上准确率最高的那一个最终确定作为K值。

#### 3、距离的计算

在KNN算法中，还有一个重要的计算就是关于距离的度量。两个样本点之间的距离代表了这两个样本之间的相似度。距离越大，差异性越大；距离越小，相似度越大。

关于距离的计算方式有下面五种方式：

1. 欧氏距离

   **欧氏距离**是我们最常用的距离公式，也叫做欧几里得距离。在二维空间中，两点的欧式距离就是：

   <img src="https://static001.geekbang.org/resource/image/f8/80/f8d4fe58ec9580a4ffad5cee263b1b80.png" alt="欧氏距离" style="zoom:33%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

   同理，我们也可以求得两点在n维空间中的距离：

   <img src="https://static001.geekbang.org/resource/image/40/6a/40efe7cb4a2571e55438b55f8d37366a.png" alt="多维欧氏距离" style="zoom:33%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

2. 曼哈顿距离

   **曼哈顿距离**在几何空间中用的比较多。以下图为例，绿色的直线代表两点之间的欧式距离，而红色和黄色的线为两点的曼哈顿距离。所以曼哈顿距离等于两个点在坐标系上绝对轴距总和。用公式表示就是：

   <img src="https://static001.geekbang.org/resource/image/bd/aa/bda520e8ee34ea19df8dbad3da85faaa.png" alt="曼哈顿距离" style="zoom:33%;box-shadow:rgba(0,0,0,0) 0 1px 5px 0px;" />

3. 闵可夫斯基距离；

4. 切比雪夫距离；

5. 余弦距离；

#### 4、KD树

其实从上文你也能看出来，KNN的计算过程是大量计算样本点之间的距离。为了减少计算距离次数，提升KNN的搜索效率，人们提出了KD树（K-Dimensional的缩写）。KD树是对数据点在K维空间中划分的一种数据结构。在KD树的构造中，每个节点都是k维数值点的二叉树。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。

#### 5、如何在sklearn中使用KNN

在Python的sklearn工具包中有KNN算法。KNN既可以做分类器，也可以做回归。如果是做分类，你需要引用：

```
from sklearn.neighbors import KNeighborsClassifier
```

如果是做回归，你需要引用：

```
from sklearn.neighbors import KNeighborsRegressor
```

从名字上你也能看出来Classifier对应的是分类，Regressor对应的是回归。一般来说如果一个算法有Classifier类，都能找到相应的Regressor类。比如在决策树分类中，你可以使用DecisionTreeClassifier，也可以使用决策树来做回归DecisionTreeRegressor。

### 五、AdaBoost

#### 1、AdaBoost定义

​	Adaboost 在训练中建立了一个联合的分类模型。boost 在英文中代表提升的意思，所以 Adaboost 是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类器。AdaBoost 算法与随机森林算法一样都属于分类算法中的集成算法。集成算法通常有两种方式，分别是投票选举（bagging）和再学习 （boosting）。

+ ​	投票选举：类似把专家召集到一个会议桌前，当做一个决定的时候， 让 K 个专家（K 个模型）分别进行分类，然后选择出现次数最多的那个类作为最终的分类结果。
+ ​	再学习：相当于把 K 个专家（K 个分类器）进行加权融合，形成一个新的超级专家 （强分类器），让这个超级专家做判断。

Adaboost是通过一系列的弱分类器根据不同的权重组合而成的。 假设弱分类器为 G<SUB>i</SUB>(x)，它在强分类器中的权重 ，那么就可以得出强分类器 f(x)：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221031195751399.png" alt="强分类器" style="zoom: 50%; box-shadow: rgba(0, 0, 0, 0) 0px 1px 5px 0px;" />

1. 每个弱分类器在强分类器中的权重是如何计算的？
2. 如何得到弱分类器，也就是在每次迭代训练的过程中，如何得到最优弱分类器？ 

#### 2、弱分类器权重的计算

实际上在一个由 K 个弱分类器中组成的强分类器中，如果弱分类器的分类效果好，那么权重应该比较大，如果弱分类器的分类效果一般，权重应该降低。所以我们需要基于这个弱分类器对样本的分类错误率来决定它的权重，其中e<SUB>i</SUB> 代表第 i 个分类器的分类错误率。用公式表示就是：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221031200339325.png" alt="弱分类器权重" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0) 0px 1px 5px 0px;" />

#### 3、在训练迭代中选择最优弱分类器

​	实际上，AdaBoost 算法是通过改变样本的数据分布来实现选择最优弱分类器的。AdaBoost 会判断每次训练的样本是否正确分类，对于正确分类的样本，降低它的权重，对于被错误分类的样本， 增加它的权重。再基于上一次得到的分类准确率，来确定这次训练样本中每个样本的权 重。然后将修改过权重的新数据集传递给下一层的分类器进行训练。这样做的好处就是， 通过每一轮训练样本的动态权重，可以让训练的焦点集中到难分类的样本上，最终得到的弱分类器的组合更容易得到更高的分类准确率。

我们可以用D<SUB>k+1</SUB>代表第 k+1 轮训练中，样本的权重集合，其中W<SUB>k+1,k</SUB>代表第 k+1 轮 中第一个样本的权重，以此类推W<SUB>k+1,N</SUB>代表第 k+1 轮中第 N 个样本的权重，因此用公式表示为：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221031201305653.png"  style="zoom:50%;box-shadow: rgba(0, 0, 0, 0) 0px 1px 5px 0px;" />

第 k+1 轮中的样本权重，是根据该样本在第 k 轮的权重以及第 k 个分类器的准确率而 定，具体的公式为：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221031201330118.png"  style="zoom:50%;box-shadow: rgba(0, 0, 0, 0) 0px 1px 5px 0px;" />

### 六、K-Means

#### 1、K-Means原理

​	K-Means 算法是一个聚类算法。你可以这么理解，最终我想把物体划分成 K 类。假设每 个类别里面，都有个“中心点”，即意见领袖，它是这个类别的核心。现在我有一个新点 要归类，这时候就只要计算这个新点与 K 个中心点的距离，距离哪个中心点近，就变成了 哪个类别。

1. 选取K个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的；
2. 将每个点分配到最近的类中心点，这样就形成了K个类，然后重新计算每个类的中心点；
3. 重复第二步，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束。

#### 2、K-Means和KNN的区别

我们可以从三个维度来区分K-Means和KNN这两个算法：

+ 首先，这两个算法解决数据挖掘的两类问题。K-Means是聚类算法，KNN是分类算法。
+ 这两个算法分别是两种不同的学习方式。K-Means是非监督学习，也就是不需要事先给出分类标签，而KNN是有监督学习，需要我们给出训练数据的分类标识。
+ 最后，K值的含义不同。K-Means中的K值代表K类。KNN中的K值代表K个最接近的邻居。

#### 3、K-Means的缺陷

K-Means聚类有个缺陷：聚类个数K值需要事先指定。如果你不知道该聚成几类，那么最好会给K值多设置几个，然后选择聚类结果最好的那个值。

### 七、EM

#### 1、EM 算法

EM 算法也叫最大期望算法，是求参数的最大似然估计的一种方法,最大似然也就是求大可能性。

原理是这样的：假设 我们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可 以得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此 得到 B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止,EM 算法经常用于聚类和机器学习领域中。

K-Means是通过距离来区分样本之间的差别的，且每个样本在计算的时候只能属于一个分类，称之为是硬聚类算法。而EM聚类在求解的过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做软聚类算法。

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221101094338194.png" alt="K-Means聚类效果" style="zoom:33%;box-shadow:rgba(0,0,0,0) 0 0px 0px 0px;" />



<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221101094518381.png" alt="EM算法聚类效果" style="zoom: 33%;box-shadow:rgba(0,0,0,0) 0 0px 0px 0px;" />



​	EM算法相当于一个聚类框架，里面有不同的聚类模型，比如GMM高斯混合模型，或者HMM隐马尔科夫模型。

#### 2、GMM高斯混合模型

GMM是通过概率密度来进行聚类，聚成的类符合高斯分布（正态分布）

#### 3、HMM隐马尔科夫模型

HMM用到了马尔可夫过程，在这个过程中，我们通过状态转移矩阵来计算状态转移的概率。HMM在自然语言处理和语音识别领域中有广泛的应用。

### 八、Apriori

#### 1、Apriori 

Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集 （frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全 等领域中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系。

+ 支持度：是个百分比，它指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大
+ 置信度：是个条件概念，就是说在A发生的情况下，B发生的概率是多少
+ 提升度：代表的是“商品A的出现，对商品B的出现概率提升的程度。

+ 频繁项集：小于最小值支持度的项目就是非频繁项集，而大于等于最小支持度的项集就是频繁项集。

我们刚完成了Apriori算法的模拟，你能看到Apriori在计算的过程中有以下几个缺点：

1. 可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了；
2. 每次计算都需要重新扫描数据集，来计算每个项集的支持度。

所以Apriori算法会浪费很多计算空间和计算时间，为此人们提出了FP-Growth算法

#### 2、FP-Growth

FP-Growth的特点是：

1. 创建了一棵FP树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。我稍后会讲解如何构造一棵FP树；
2. 整个生成过程只遍历数据集2次，大大减少了计算量。

所以在实际工作中，我们常用FP-Growth来做频繁项集的挖掘，

**1.创建项头表（item header table）**

创建项头表的作用是为FP构建及频繁项集挖掘提供索引。

这一步的流程是先扫描一遍数据集，对于满足最小支持度的单个项（K=1项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。

项头表包括了项目、支持度，以及该项在FP树中的链表。初始的时候链表为空。

**2.构造FP树**

FP树的根节点记为NULL节点。

整个流程是需要再次扫描数据集，对于每一条数据，按照支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。

**3.通过FP树挖掘频繁项集**

到这里，我们就得到了一个存储频繁项集的FP树，以及一个项头表。我们可以通过项头表来挖掘出每个频繁项集。

具体的操作会用到一个概念，叫“条件模式基”，它指的是以要挖掘的节点为叶子节点，自底向上求出FP子树，然后将FP子树的祖先节点设置为叶子节点之和。

### 九、PageRank

​		PageRank 起源于论文影响力的计算方式，如果一篇文论被引入的次数越多，就代表这篇论文的影响力越强。同样 PageRank 被 Google 创造性地应用到了网页权重的计算中：当 一个页面链出的页面越多，说明这个页面的“参考文献越多，当这个页面被链入的频率 越高，说明这个页面被引用的次数越高。基于这个原理，我们可以得到网站的权重划分。

#### 1、PageRank 的原理

简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，出链指的是链接出去的链接，入链指的是链接进来的链接。u 为待评估的页面， B<SUB>u</SUB>为页面 u 的入链集合，用公式表示为：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221101121252832.png" alt="网页的影响力" style="zoom: 30%;box-shadow: rgba(0, 0, 0, 0) 0px 1px 5px 0px;" />

针对入链集合中的任意页面 v，它能给 u 带来的影响力是其自身的影响力 PR(v) 除以 v 页面的出链数量，即页面 v 把影响力 PR(v) 平均分配给了它的出链，这样统计所有能给 u 带来链接的页面 v，得到的总和就是网页 u 的影响力，即为 PR(u)。

举个例子，假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221101122034940.png" alt="网页链接关系" style="zoom:23%;" />

 A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的 时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。 B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。 这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221101122140825.png"  style="zoom: 50%; box-shadow: rgba(0, 0, 0, 0) 0px 1px 5px 0px;" />

我们假设 A、B、C、D 四个页面的初始影响力都是相同的，即：

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221101122354942.png"  style="zoom:45%;box-shadow: rgba(0, 0, 0, 0) 0px 1px 5px 0px;" />

当进行第一次转移之后，各页面的影响力

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221101122439293.png"  style="zoom:50%;box-shadow: rgba(0, 0, 0, 0) 0px 1px 5px 0px;" />

然后我们再用转移矩阵乘以得到结果，直到第 n 次迭代后 影响力不再发生变化，可以收敛到 (0.3333，0.2222，0.2222，0.2222，也就是对应着 A、B、C、D 四 个页面最终平衡状态下的影响力。 你能看出 A 页面相比于其他页面来说权重更大，也就是PR值更高。而 B、C、D 页面的 PR 值相等。

#### 2、PageRank 的随机浏览模型

PageRank 可能出现的问题

1. 等级泄露（Rank Leak）：如果一个网页没有出链，就像是一个黑洞一样，吸收了其他 网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。
2. 等级沉没（Rank Sink）：如果一个网页只有出链，没有入链（如下图所示），计算的 过程迭代下来，会导致这个网页的 PR 值为 0（也就是不存在公式中的 V）。

为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型，他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一 种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直 接输入网址访问其他页面，虽然这个概率比较小。 所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取 一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比 如直接输入网址。

<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/image-20221101123221224.png" alt="随机浏览模型" style="zoom: 33%;box-shadow: rgba(0, 0, 0, 0) 0px 1px 5px 0px;" />

其中 N 为网页总数，这样我们又可以重新迭代网页的权重计算了，因为加入了阻尼因子 d，一定程度上解决了等级泄露和等级沉没的问题。 通过数学定理也可以证明，最终 PageRank 随机浏览模型是可以收敛 的，也就是可以得到一个稳定正常的 PR 值。

### 十、随机森林（补充）

随机森林的英文是 Random Forest，英文简写是 RF。它实际上是一个包含多个决策树的分类 器，每一个子分类器都是一棵 CART 分类回归树。所以随机森林既可以做分类，又可以做回归。

+ 当它做分类的时候，输出结果是每个子分类器的分类结果中最多的那个。你可以理解是每个分类器都做投票，取投票最多的那个结果。
+ 当它做回归的时候，输出结果是每棵 CART 树的回归结果 的平均值。

### 推荐深度学习训练平台

+ 免费GPU首推kaggle！p100 16G每周至少30小时，比Colab免费分配的Tesla T4快了好几倍。

  kaggle上除了不能改python版本和存储输出文件比较麻烦外，真的很良心了

+ 避坑Colab pro（在淘宝充的80多一个月）分配的GPU（Tesla v100 sxm2 ）开到高配都和p100相差不大，体感甚至弱一些，而且每个月都有时间（算力）限制，注意是每个月（开最高配置，没仔细算过，估计12小时都跑不到。。性价比极低）

+ [Featurize](https://featurize.cn/?s=d7ce99f842414bfcaea5662a97581bd1)